# Naestro providers: multi-node DGX Spark (local-first) + cloud spillover

inference:
  local:
    # DGX A
    - id: llama70b_judge_dgx_a
      runtime: tensorrt_llm
      model: llama-3.1-70b-instruct-fp8
      endpoint: http://dgx-a:8001
      max_batch: 4
      kv_cache: fp8
      roles: [judge, proposer]

    - id: deepseek32b_dgx_a
      runtime: vllm
      model: deepseek-v3.2-32b-instruct
      endpoint: http://dgx-a:8002
      max_batch: 8
      roles: [proposer, synthesizer]

    - id: qwen32b_awq_dgx_a
      runtime: vllm
      model: qwen-3-32b-instruct-awq
      endpoint: http://dgx-a:8003
      max_batch: 12
      roles: [critic, code]

    # DGX B
    - id: llama70b_judge_dgx_b
      runtime: tensorrt_llm
      model: llama-3.1-70b-instruct-fp8
      endpoint: http://dgx-b:8001
      max_batch: 4
      kv_cache: fp8
      roles: [judge, proposer]

    - id: deepseek32b_dgx_b
      runtime: vllm
      model: deepseek-v3.2-32b-instruct
      endpoint: http://dgx-b:8002
      max_batch: 8
      roles: [proposer, synthesizer]

    - id: qwen32b_awq_dgx_b
      runtime: vllm
      model: qwen-3-32b-instruct-awq
      endpoint: http://dgx-b:8003
      max_batch: 12
      roles: [critic, code]

    # Apple OpenELM router provider (local OpenAI-compatible endpoint)
    - id: apple_openelm_router
      cell: local-01
      runtime: openai
      model: apple/OpenELM-3B-Instruct
      endpoint: http://127.0.0.1:8602/v1
      roles: [router, proposer, critic]
      max_output_tokens: 4096

  cloud:
    - id: openai_gpt4x
      provider: openai
      model: gpt-4.1
      auth_env: OPENAI_API_KEY
      roles: [overflow, long_context]

    - id: anthropic_claude37
      provider: anthropic
      model: claude-3-7-opus
      auth_env: ANTHROPIC_API_KEY
      roles: [judge, safety_backup]

    - id: google_gemini25
      provider: google
      model: gemini-2.5-pro
      auth_env: GOOGLE_API_KEY
      roles: [long_context]

    - id: baidu_ernie_x1_1
      provider: baidu
      model: ernie-x1.1
      auth_env: BAIDU_API_KEY
      roles: [reasoning, verification]

routing:
  policy: local_first_with_cloud_spill  # Cloud Pool includes ERNIE X1.1 (Baidu Qianfan) for reasoning/verification, subject to API access, latency, and cost
  defaults:
    judge: llama70b_judge_dgx_a
    proposer: deepseek32b_dgx_a
    critic: qwen32b_awq_dgx_a
    synthesizer: deepseek32b_dgx_a
  fallbacks:
    judge: [llama70b_judge_dgx_b, anthropic_claude37, openai_gpt4x]
    proposer: [deepseek32b_dgx_b, openai_gpt4x]
    critic: [qwen32b_awq_dgx_b, openai_gpt4x]
    synthesizer: [deepseek32b_dgx_b, google_gemini25]
  limits:
    max_concurrency: 16
    max_tokens_per_run: 120000
    long_context_threshold: 200000

retrieval:
  vector_store:
    kind: qdrant
    url: http://127.0.0.1:6333
    collection: naestro_docs
  embeddings:
    local: bge-large
    cloud_fallback: text-embedding-3-large

planner:
  engine: langgraph   # or dspy / none
  timeout_ms: 60000

evaluation:
  rag: ragas
  general: deepeval

safety:
  primary: nemo_guardrails
  pii_filters: [email, phone, credit_card]

